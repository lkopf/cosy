"""
This file is meant to try different prompts on the same set of images, to see which concepts are generated by each prompt.

So the results will show the results of different promtps on the same set of images.
Then store:
- which prompt is used for which concept
- what are the x images used (should be the same every time)

The idea is that we use 3-4 sets of images, vary the number of images used and use the 5 prompts.

This gets saved in a folder with the image concept as a title
- a text file per prompt with the concept inside as well 

to do:
- select a few node numbers to check the prompt outcomes from?

"""

import os
import time 

import argparse
import asyncio
import torch 

from utils import retrieve_top_x_indices, prompting, response_to_csv, set_openai_api_key


PROMPTS = {
    '0': (    "You are a visual concept tagger and compressor. Given a group of images that activate the same neuron, your job is to extract a single, specific visual concept that appears prominently in all of them. This concept must be: - Concrete (e.g., 'red double-decker bus', not 'transportation') - Visual (can be drawn by an image generation model like SDXL) - Consistent (appears in *every* image). Avoid vague or abstract themes like 'diversity', 'context', or 'life'. Focus on visible entities (objects, creatures, materials, or actions). Output only the final concept that occurs in all images, no other text."),

    '1':(
    "You are helping an image generation model recreate the core visual theme behind a neuron activation.\n"
    "You are given images that all highly activate the same neuron.\n"
    "Your job is to describe the shared visual feature among them that, if used as an image generation prompt, would likely cause the neuron to fire again.\n\n"
    "The concept must be:\n"
    "- Visually specific (e.g., 'wooden pier over a lake', not 'nature')\n"
    "- Immediately imageable\n"
    "- Described in 5-8 words max\n\n"
    "Output only the visual concept that occurs in all images, no other text."
),
    '2': (
    "You are identifying what sets a group of images apart in terms of visual content.\n"
    "The images you are given strongly activate the same neuron.\n"
    "Identify the one visually concrete and discriminative concept that appears across them all.\n\n"
    "Your answer should:\n"
    "- Be usable as a concept for image generation in SDXL\n"
    "- Be free of vague or abstract language\n"
    "- Refer to specific objects, actions, or textures\n\n"
    "Examples: 'aerial view of a sunflower field', 'person holding a red umbrella in rain', 'close-up of reptile skin'\n\n"
    "Only output the final visual concept, that occurs in all images, no other text.."
),
    '3':(
    "You are identifying low-level visual features that are consistently present in a group of images.\n"
    "These features cause strong activation in an early-layer neuron of a vision model.\n\n"
    "Your goal is to extract a single shared perceptual feature that appears in all images.\n\n"
    "Focus on things like:\n"
    "- Color patches (e.g., 'bright orange areas')\n"
    "- Repeating textures (e.g., 'brick pattern')\n"
    "- Simple geometric shapes (e.g., 'horizontal lines', 'circle clusters')\n"
    "- Lighting effects (e.g., 'specular reflections', 'soft shadows')\n"
    "- Material appearance (e.g., 'glossy surface', 'matte plastic')\n\n"
    "Avoid abstract ideas or scene-level objects.\n"
    "Output only the specific visual feature or texture, that occurs in all images, no other text.."
),
    '4': (
    "You are analyzing a set of images that all strongly activate the same neuron in a visual neural network.\n"
    "Your goal is to identify the single shared visual feature that causes this activation.\n\n"
    "Infer whether the neuron is likely from an early or later layer based on the nature of the common patterns:\n"
    "- If the images share a low-level visual trait (e.g., color, texture, shape), describe that precise visual feature.\n"
    "- If the images share an object, environment, or action, describe that as a concrete, imageable concept.\n\n"
    "Focus on:\n"
    "- Specific textures (e.g., 'cracked stone texture')\n"
    "- Local patterns (e.g., 'horizontal blue lines')\n"
    "- Mid-level parts (e.g., 'car headlights', 'tree branches')\n"
    "- Full objects or scenes (e.g., 'yellow school bus on a street')\n\n"
    "Avoid abstract or interpretive descriptions like 'diversity', 'context', or 'daily life'.\n"
    "Do not speculate beyond what is visually present.\n"
    "Your answer should be short (5â€“10 words) and directly usable as a concept for SDXL.\n\n"
    "Output only the final visual concept, that occurs in all images, no other text.."
)

}

IMAGE_NUMBER = [5,10,15,20]

LAYERS = [1,2,3,4]


def main():

    for layer in LAYERS:

        print("Layer ", layer)

        activation_path =f'/cosy/src/LLM_activations/activations/val_resnet18-layer{layer}.pt'

        # Load the activations 
        print(f"Loading activations from: {activation_path}")
        activations = torch.load(activation_path)
        print("Activation shape:", activations.shape)

        for prompt_number, prompt in PROMPTS.items(): 

            print("Prompt number ", prompt_number)
            print(prompt)
        
            # retrieve top x images for each of the activations. 
            for image_number in IMAGE_NUMBER:
                top_x_indices = retrieve_top_x_indices(activations, image_number)
                print("Top-X indices shape:", top_x_indices.shape)

                # take only 10 neurons
                top_x_ten = top_x_indices[:10,:]
                print("Top 10 neurons shape:", top_x_ten.shape)

                # Prompting to GPT API
                start_time = time.time()
                responses = asyncio.run(prompting(top_x_indices=top_x_ten, prompt=prompt))
                print("Number of responses:", len(responses))

                # Convert to CSV
                response_to_csv(responses, layer=layer, top_x=image_number, prompt_number=prompt_number)
                print("CSV file created.")
                print("Time taken:", time.time() - start_time)

            



if __name__ == "__main__":

    try:

        parser = argparse.ArgumentParser(description="Process neuron activations and save responses to CSV.")
        parser.add_argument("--apikey", type=str, default="APIKEY", help="API key for OpenAI GPT4 (default: API). Not needed if you have the key in your environment")

        args = parser.parse_args()

        set_openai_api_key(fallback_key=args.apikey)
        main()
    
    except:
        apikey = "INSERT YOUR API KEY HERE"
        set_openai_api_key(fallback_key=apikey)

        main()


